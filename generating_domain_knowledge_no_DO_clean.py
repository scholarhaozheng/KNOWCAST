Generating_Metro_Related_data_or_not = FalseGenerating_logit_probabilities_or_not = FalseGet_Time_DepartFreDic_or_not = FalseConnect_to_SQL_or_not = Falsegenerate_OD_DO_array_or_not = Falsetrip_distribution_or_not = Falseimport sysimport multiprocessingfrom metro_data_convertor.Find_project_root import Find_project_rootfrom datetime import timedeltaif Generating_Metro_Related_data_or_not:    from dmn_knw_gnrtr.Generating_Metro_Related_data import Generating_Metro_Related_data    from metro_data_convertor.Generating_logit_probabilities import Generating_logit_probabilities    from metro_data_convertor.Get_Time_DepartFreDic import Get_Time_DepartFreDic    from metro_data_convertor.Process_Time_DepartFreDic import Process_Time_DepartFreDicfrom dmn_knw_gnrtr.fit_trip_distribution_model import fit_trip_distribution_modelfrom dmn_knw_gnrtr.generating_OD_section_pssblty_sparse_array_0209 import generating_OD_section_pssblty_sparse_array_0209import pickleimport numpy as npimport yamlimport osimport argparsefrom archieve.train_button import read_cfg_fileimport timefrom dmn_knw_gnrtr.generating_array_OD import generate_OD_DO_arrayfrom dmn_knw_gnrtr.generating_array_OD import generating_trip_generation_data_and_OD_dictfrom dmn_knw_gnrtr.generating_array_OD import Connect_to_SQLfrom dmn_knw_gnrtr.generating_domain_knowledge_with_timestamp import \    generating_traffic_assignment_knowledge_with_timestampfrom dmn_knw_gnrtr.PYGT_signal_generation_one_hot import PYGT_signal_generationfrom dmn_knw_gnrtr.run_PYGT_0917 import run_PYGTfrom dmn_knw_gnrtr.test_PYGT_0917 import test_PYGTproject_root_main = os.path.dirname(os.path.abspath(__file__))  # 获取当前脚本的绝对路径if project_root_main not in sys.path:    sys.path.insert(0, project_root_main)def main():    time_start = time.time()    start_time_str = time.strftime("%Y%m%d_%H%M%S", time.localtime(time_start))    script_name = os.path.basename(__file__).split('.')[0]    file_name = f"{start_time_str}_{script_name}.txt"    def write_cfg_file(filename, cfg):        with open(filename, 'w') as ymlfile:            yaml.dump(cfg, ymlfile, default_flow_style=False, allow_unicode=True)    args = argparse.Namespace(        config_filename=f'data{os.path.sep}config{os.path.sep}train_sz_dim26_units96_h4c512_250503.yaml')    cfg = read_cfg_file(args.config_filename)    project_root = Find_project_root()    text_place_and_month = "suzhou_03_trimmed"    base_dir = os.path.join(project_root, f"data{os.path.sep}{text_place_and_month}")    station_manager_dict_name='station_manager_dict_no_11.pkl'    graph_sz_conn_no_name='graph_sz_conn_no_11.pkl'    start_date_str = cfg['domain_knowledge']['start_date_str']    end_date_str = cfg['domain_knowledge']['end_date_str']    everyday_start_time_str = cfg['domain_knowledge']['everyday_start_time_str']    everyday_end_time_str = cfg['domain_knowledge']['everyday_end_time_str']    train_filename = os.path.join(base_dir, 'train_dict.pkl')    train_dict_file_path = os.path.join(base_dir, 'train_dict.pkl')    OD_path_visit_prob_dic_file_path = os.path.join(base_dir, 'OD_path_visit_prob_dic.pkl')    Time_DepartFreDic_file_path = os.path.join(base_dir, 'Time_DepartFreDic.pkl')    Time_DepartFreDic_Array_file_path = os.path.join(base_dir, 'Time_DepartFreDic_Array.pkl')    Time_DepartFreDic_filename = os.path.join(base_dir, 'Time_DepartFreDic.pkl')    suzhou_sub_data_file_path = os.path.join(base_dir, 'suzhou_sub_data.xlsx')    excel_path = os.path.join(base_dir, 'Suzhou_zhandian_no_11.xlsx')    graph_sz_conn_root = os.path.join(base_dir, f'{graph_sz_conn_no_name}')    station_index_root = os.path.join(base_dir, 'station_index_no_11.pkl')    station_manager_dict_root = os.path.join(base_dir, 'station_manager_dict_no_11.pkl')    result_API_root = os.path.join(base_dir, 'result_API_modified.xlsx')    time_interval = timedelta(minutes=int(cfg['domain_knowledge']['timedelta_minutes']))    OD_path_visit_prob_array_file_path = os.path.join(base_dir, 'OD_path_visit_prob_array.pkl')    station_manager_dict_file_path = f"{base_dir}{os.path.sep}{station_manager_dict_name}"    Date_and_time_OD_path_dic_file_path = os.path.join(base_dir, f'Date_and_time_OD_path_dic.pkl')    Date_and_time_OD_path_cp_factors_dic_file_path = os.path.join(base_dir, f'Date_and_time_OD_path_cp_factors_dic.pkl')    OD_feature_array_file_path = os.path.join(base_dir, f'OD_feature_array_dic.pkl')    OD_path_dic_file_path = os.path.join(base_dir, f'OD_path_dic.pkl')    # This is used to generate information for all paths corresponding to all OD pairs. If the road network remains unchanged, there is no need to run it.    if Generating_Metro_Related_data_or_not:        Generating_Metro_Related_data(excel_path, graph_sz_conn_root, station_index_root, station_manager_dict_root,result_API_root,                                      suzhou_sub_data_file_path, Time_DepartFreDic_filename, time_interval,                                      Time_DepartFreDic_file_path, Time_DepartFreDic_Array_file_path, OD_path_visit_prob_array_file_path,                                      train_dict_file_path, OD_path_visit_prob_dic_file_path, train_filename, start_date_str, end_date_str)    # Used to generate a logit distribution result under a specific parameter setting, format: (154, 154, 3, 1).    if Generating_logit_probabilities_or_not:        Generating_logit_probabilities(train_dict_file_path, OD_path_visit_prob_dic_file_path,                                               station_manager_dict_file_path, graph_sz_conn_root, station_manager_dict_root, station_index_root, result_API_root)    # Create a dictionary with timestamps as keys and subway departure frequency information as values.    if Get_Time_DepartFreDic_or_not:        Get_Time_DepartFreDic(suzhou_sub_data_file_path, Time_DepartFreDic_filename, time_interval,                          excel_path, graph_sz_conn_root, station_index_root, start_date_str, end_date_str, station_manager_dict_root, result_API_root)        for prefix in ("train","test","val"):            Time_DepartFreDic_Array_file_path = os.path.join(project_root, base_dir, f'{prefix}_Time_DepartFreDic_Array.pkl')            Process_Time_DepartFreDic(station_index_root, Time_DepartFreDic_file_path, Time_DepartFreDic_Array_file_path, prefix)        # Combine Time_DepartFreDic to obtain a time-uarying OD_visiting_prob matrix, which is deprecated in the current version.        # Reprocessing_OD_visiting_prob(OD_path_visit_prob_dic_file_path, OD_path_visit_prob_array_file_path)    generating_OD_section_pssblty_sparse_array_0209_or_not = False    if generating_OD_section_pssblty_sparse_array_0209_or_not:        generating_OD_section_pssblty_sparse_array_0209(base_dir, station_manager_dict_name,                                            Time_DepartFreDic_file_path, OD_path_dic_file_path,                                            station_manager_dict_file_path, OD_feature_array_file_path,                                            Date_and_time_OD_path_cp_factors_dic_file_path)    train_sql = cfg['domain_knowledge']['train_sql']    test_sql = cfg['domain_knowledge']['test_sql']    val_sql = cfg['domain_knowledge']['val_sql']    repeated_or_not_repeated = "not_repeated"    db_config_path = os.path.join(project_root, 'data', 'config', 'db_config.yaml')    with open(db_config_path, 'r') as f:        db_creds = yaml.safe_load(f)    host = db_creds['host']    user = db_creds['user']    password = db_creds['password']    database = db_creds['database']    # RGCN is trained independently.    Using_lat_lng_or_index = cfg['domain_knowledge']['Using_lat_lng_or_index']    if Using_lat_lng_or_index == "lat_lng":  #If latitude and longitude are used, the number of node features is num_nodes + 2; otherwise, it is num_nodes + 1.        RGCN_node_features = cfg['model']['num_nodes'] + 3    else:        RGCN_node_features = cfg['model']['num_nodes'] + 2    RGCN_hidden_units = int(cfg['domain_knowledge']['RGCN_hidden_units'])    RGCN_output_dim = int(cfg['domain_knowledge']['RGCN_output_dim'])    RGCN_K = int(cfg['domain_knowledge']['RGCN_K'])    lr = int(cfg['domain_knowledge']['lr'])    epoch_num = int(cfg['domain_knowledge']['epoch_num'])    seq_len = cfg['model']['seq_len']    train_ratio = float(cfg['domain_knowledge']['train_ratio'])    input_dim_m_1 = cfg['model']['input_dim'] - 1    initial_gamma = float(cfg['domain_knowledge']['initial_gamma'])    lr_gener = float(cfg['domain_knowledge']['lr_gener'])    maxiter = int(cfg['domain_knowledge']['maxiter'])    for prefix in ("train", "test", "val"):        result_array_file_path = os.path.join(base_dir, f'{prefix}_result_array.pkl')        station_manager_dict_file_path = os.path.join(base_dir, f"{station_manager_dict_name}")        normalization_params_file_path = os.path.join(f'data{os.path.sep}{text_place_and_month}', 'normalization_params.pkl')        if Connect_to_SQL_or_not:            df,x_y_time,sz_conn_to_station_index,station_index_to_sz_conn=Connect_to_SQL(prefix, train_sql, test_sql, val_sql, station_manager_dict_file_path,                                                                                         host, user, password, database)        if generate_OD_DO_array_or_not:            result_array_OD = generate_OD_DO_array(df, x_y_time, sz_conn_to_station_index, station_index_to_sz_conn, base_dir, prefix, station_manager_dict_file_path, result_array_file_path)        if trip_distribution_or_not:            if (prefix == "train"):                with open(station_manager_dict_file_path, 'rb') as f:                    station_manager_dict = pickle.load(f)                with open(result_array_file_path, 'rb') as f:                    mid_dic_for_trip_generation = pickle.load(f, errors='ignore')                T_N_D_ODs_dic = mid_dic_for_trip_generation['T_N_D_ODs']                O_data_dic = mid_dic_for_trip_generation['Trip_Production_In_Station_or_Out_Station']                D_data_dic = mid_dic_for_trip_generation['Trip_Attraction_In_Station_or_Out_Station']                sorted_times = sorted(T_N_D_ODs_dic.keys())                T_N_D_OD_array = np.array(                    [T_N_D_ODs_dic[time] for time in sorted_times])                O_data = np.array([O_data_dic[time] for time in sorted_times])                D_data = np.array([D_data_dic[time] for time in sorted_times])                C_data = station_manager_dict['station_distance_matrix']                q_obs_data = T_N_D_OD_array                time_steps = len(O_data)                # The gravity model is trained independently.                optimal_gamma, a_fitted, b_fitted, q_predicted_list = fit_trip_distribution_model(O_data, D_data, C_data,                                                                                                  q_obs_data,                                                                                                  time_steps=time_steps,                                                                                                  initial_gamma=initial_gamma,                                                                                                  lr_gener=lr_gener,                                                                                                  maxiter=maxiter)                save_to_cfg = False                if save_to_cfg:                    config = read_cfg_file(args.config_filename)                    config['trip_distribution']['gamma'] = optimal_gamma                    config['trip_distribution']['a'] = a_fitted                    config['trip_distribution']['b'] = b_fitted                    write_cfg_file(args.config_filename, config)                pkl_filename = os.path.join(project_root, f"data{os.path.sep}{text_place_and_month}",                                            'trip_generation_trained_params.pkl')                trained_params = {                    'gamma': optimal_gamma,                    'a': a_fitted,                    'b': b_fitted,                }                def save_to_pkl(data, filename):                    with open(filename, 'wb') as file:                        pickle.dump(data, file)                save_to_pkl(trained_params, pkl_filename)        for str_prdc_attr in ("prdc", "attr"):            trip_generation_array_file_path = os.path.join(base_dir, f'{prefix}_result_array.pkl')            PYGT_signal_generation(base_dir, prefix, station_manager_dict_name, graph_sz_conn_no_name,                                   station_manager_dict_file_path, graph_sz_conn_root,                                   trip_generation_array_file_path,                                   normalization_params_file_path, str_prdc_attr, Using_lat_lng_or_index)            if (prefix == "train"):                run_PYGT(base_dir, prefix, str_prdc_attr, RGCN_node_features, RGCN_hidden_units, RGCN_output_dim,                         RGCN_K, lr, epoch_num, train_ratio)                test_PYGT(base_dir, prefix, str_prdc_attr)            generating_trip_generation_data_and_OD_dict(input_dim_m_1, base_dir, prefix, str_prdc_attr, seq_len, everyday_start_time_str, everyday_end_time_str)#生成train{'finished','unfinished','y','xtime','ytime'}        generating_traffic_assignment_knowledge_with_timestamp(base_dir, prefix, repeated_or_not_repeated, seq_len)    time_stop = time.time()    print(f"Time used: {time_stop - time_start} seconds.")    duration = time_stop - time_start    with open(file_name, 'w') as f:        f.write(f"Start: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time_start))}\n")        f.write(f"Stop: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time_stop))}\n")        f.write(f"Time: {duration:.2f} s\n")    print(f"Time used: {time_stop-time_start} seconds.")if __name__ == "__main__":    multiprocessing.set_start_method('spawn', force=True)    main()